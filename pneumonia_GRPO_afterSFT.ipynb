{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Image as HFImage\n",
    "\n",
    "def build_split_df(split_dir: str, normal_sub=\"NORMAL\", pneu_sub=\"PNEUMONIA\"):\n",
    "    rows = []\n",
    "    img_exts = (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\")\n",
    "    # NORMAL\n",
    "    for p in glob.glob(os.path.join(split_dir, normal_sub, \"*\")):\n",
    "        if os.path.isfile(p) and p.lower().endswith(img_exts):\n",
    "            rows.append({\"image_path\": p, \"label\": \"normal\"})\n",
    "    # PNEUMONIA\n",
    "    for p in glob.glob(os.path.join(split_dir, pneu_sub, \"*\")):\n",
    "        if os.path.isfile(p) and p.lower().endswith(img_exts):\n",
    "            rows.append({\"image_path\": p, \"label\": \"pneumonia\"})\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(f\"No images found under: {split_dir}\")\n",
    "    return df\n",
    "\n",
    "def make_hf_dataset_from_dir(root_dir: str):\n",
    "    \"\"\"\n",
    "    root_dir/\n",
    "      train/\n",
    "        NORMAL/\n",
    "        PNEUMONIA/\n",
    "      val/\n",
    "        NORMAL/\n",
    "        PNEUMONIA/\n",
    "      test/\n",
    "        NORMAL/\n",
    "        PNEUMONIA/\n",
    "    \"\"\"\n",
    "    splits = {}\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        split_path = os.path.join(root_dir, split)\n",
    "        df = build_split_df(split_path)\n",
    "\n",
    "        ds = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "        ds = ds.map(lambda ex: {\"image\": ex[\"image_path\"]})\n",
    "\n",
    "        ds = ds.cast_column(\"image\", HFImage())\n",
    "\n",
    "        splits[split] = ds\n",
    "\n",
    "    return splits\n",
    "\n",
    "# ÏòàÏãú ÏÇ¨Ïö©\n",
    "data_root = \"/workspace/kosombe2025/datasets/paultimothymooney/chest-xray-pneumonia/versions/2/chest_xray\"  \n",
    "train_ds, val_ds, test_ds = datasets_dict[\"train\"], datasets_dict[\"val\"], datasets_dict[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, Image as HFImage\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "model_id  = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "cache_dir = \"/workspace/huggingface/models/\"  \n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Your task:\n",
    "1. Think through the question step by step, and enclose your reasoning process inside <think>...</think> tags.\n",
    "2. Then provide ONLY the final answer - either \"yes\" or \"no\" - inside <answer>...</answer> tags, written in lowercase letters.\n",
    "3. Do not include anything else outside of these tags.\"\"\"\n",
    "\n",
    "QUESTION_POOL = [\n",
    "    \"Does this chest X-ray show pneumonia?\",\n",
    "    \"Is there evidence of pneumonia in this X-ray?\",\n",
    "    \"Does the scan indicate pneumonia?\",\n",
    "    \"Can you see signs of pneumonia in this image?\",\n",
    "    \"Is pneumonia present in the chest X-ray?\",\n",
    "    \"Does this image suggest a pneumonia diagnosis?\",\n",
    "    \"Is pneumonia detected in this chest X-ray image?\",\n",
    "    \"Does the chest radiograph reveal pneumonia?\",\n",
    "]\n",
    "def random_question() -> str:\n",
    "    return random.choice(QUESTION_POOL)\n",
    "\n",
    "def label_to_answer(label: str) -> str:\n",
    "    # \"pneumonia\" -> \"yes\" / \"normal\" -> \"no\"\n",
    "    return \"yes\" if str(label).lower() == \"pneumonia\" else \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"   \n",
    "SFT_DIR = \"/workspace/kosombe2025/MedVLM-SFT-Qwen2_5VL-YESNO_text_1012/final_model\"            \n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    BASE_MODEL_ID, cache_dir=cache_dir, use_fast=True, padding_side=\"left\"\n",
    ")\n",
    "tok = processor.tokenizer\n",
    "if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "    tok.pad_token = tok.eos_token  \n",
    "\n",
    "\n",
    "def looks_like_lora_adapter(path: str) -> bool:\n",
    "    return os.path.exists(os.path.join(path, \"adapter_config.json\"))\n",
    "\n",
    "def load_sft_for_grpo():\n",
    "    if looks_like_lora_adapter(SFT_DIR):\n",
    "        base = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            BASE_MODEL_ID,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(base, SFT_DIR, is_trainable=True)\n",
    "        print('load completed LoRA adapter from SFT_DIR')\n",
    "    else:\n",
    "        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            SFT_DIR,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    if getattr(model.config, \"pad_token_id\", None) is None and tok.pad_token_id is not None:\n",
    "        model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_sft_for_grpo()\n",
    "\n",
    "if hasattr(model, \"print_trainable_parameters\"):\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    print(\"Loaded full model (no LoRA adapters).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chat_and_prompt(example):\n",
    "    q = random_question()\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": q},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(\n",
    "        conversation, add_generation_prompt=True, tokenize=False\n",
    "    )\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"images\": [example[\"image\"]],\n",
    "        \"solutions\": label_to_answer(example[\"label\"]),\n",
    "    }\n",
    "\n",
    "def prepare_split_for_training(ds: Dataset):\n",
    "\n",
    "    return ds.map(to_chat_and_prompt, remove_columns=ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    pattern = r\"^<think>[\\s\\S]*?</think>\\s*<answer>\\s*(?:yes|no)\\s*</answer>\\s*$\"\n",
    "    rewards = []\n",
    "    for content in completions:\n",
    "        ok = re.match(pattern, content.strip(), flags=re.IGNORECASE)\n",
    "        rewards.append(0.2 if ok else 0.0)  \n",
    "    return rewards\n",
    "\n",
    "def _normalize(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.replace(\"yes.\", \"yes\").replace(\"no.\", \"no\")\n",
    "    return s\n",
    "\n",
    "# Ï†ïÌôïÎèÑ Î≥¥ÏÉÅ: ÎßûÏúºÎ©¥ ÌÅ¨Í≤å(1.0), ÏïÑÎãàÎ©¥ 0\n",
    "def accuracy_reward(completions: List[str], solutions: List[str], **kwargs) -> List[float]:\n",
    "    rewards = []\n",
    "    for content, sol in zip(completions, solutions):\n",
    "        sol_n = _normalize(sol)\n",
    "        m = re.search(r\"<answer>\\s*(yes|no)\\s*</answer>\", content, flags=re.IGNORECASE)\n",
    "        if not m:\n",
    "            rewards.append(-0.1)\n",
    "            continue\n",
    "        ans = _normalize(m.group(1))\n",
    "        reward = 1.0 if ans == sol_n else 0.0\n",
    "\n",
    "        think = re.search(r\"<think>([\\s\\S]*?)</think>\", content, flags=re.IGNORECASE)\n",
    "        if think:\n",
    "\n",
    "            if len(think.group(1).split()) > 60:\n",
    "                reward -= 0.05\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_reward(['<think>awonoqcoala aclalknnc </think><answer>no</answer>'],['no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_EXISTING_DATASETS = False  \n",
    "\n",
    "if not USE_EXISTING_DATASETS:\n",
    "    data_root = \"/workspace/kosombe2025/datasets/paultimothymooney/chest-xray-pneumonia/versions/2/chest_xray/chest_xray\"  \n",
    "    datasets_dict = make_hf_dataset_from_dir(data_root)\n",
    "    train_raw = datasets_dict[\"train\"]\n",
    "    val_raw   = datasets_dict[\"val\"]\n",
    "    test_raw  = datasets_dict[\"test\"]\n",
    "else:\n",
    "    raise NotImplementedError(\"train_raw/val_raw/test_rawÎ•º Î∞îÏù∏Îî©ÌïòÏÑ∏Ïöî.\")\n",
    "\n",
    "# GRPOÏö© Ï†ÑÏ≤òÎ¶¨\n",
    "train_ds = prepare_split_for_training(train_raw)\n",
    "val_ds   = prepare_split_for_training(val_raw)  \n",
    "test_ds  = prepare_split_for_training(test_raw)  \n",
    "\n",
    "print(train_ds[0].keys())  # ['prompt', 'images', 'solutions']\n",
    "\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import interleave_datasets\n",
    "yes_ds = train_ds.filter(lambda ex: ex[\"solutions\"].lower()==\"yes\").shuffle(42)\n",
    "no_ds  = train_ds.filter(lambda ex: ex[\"solutions\"].lower()==\"no\").shuffle(42)\n",
    "train_ds_bal = interleave_datasets([yes_ds, no_ds], probabilities=[0.5, 0.5], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds[0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_bal[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(\n",
    "    output_dir=\"MedVLM-R1-Qwen2.5-VL-3B-CXR-YESNO-1012-newconfig\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=6,\n",
    "    gradient_accumulation_steps=2,             \n",
    "    num_generations=6,                             \n",
    "    learning_rate=5e-6,                            \n",
    "    bf16=True,\n",
    "    max_prompt_length=None,                      \n",
    "    max_completion_length=512,                  \n",
    "    temperature=0.7,                             \n",
    "    repetition_penalty=1.05,\n",
    "    beta=0.02,                                  \n",
    "    loss_type=\"dapo\",                              \n",
    "    importance_sampling_level=\"sequence\",          \n",
    "    mask_truncated_completions=True,               \n",
    "    scale_rewards=\"none\",                       \n",
    "    top_entropy_quantile=0.2,                     \n",
    "    remove_unused_columns=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    eval_strategy=\"no\",\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "train_ds = train_ds.shuffle(seed=42)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=processor,\n",
    "    reward_funcs=[format_reward, accuracy_reward],\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "adapter_path = \"/workspace/kosombe2025/MedVLM-R1-Qwen2.5-VL-3B-CXR-YESNO-interleave-1011/checkpoint-2600\"\n",
    "\n",
    "\n",
    "cache_dir = \"/workspace/huggingface/models/\"\n",
    "\n",
    "print(\"1. Î™®Îç∏ Î∞è ÌîÑÎ°úÏÑ∏ÏÑúÎ•º Î°úÎî©Ìï©ÎãàÎã§...\")\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_model_id, cache_dir=cache_dir, use_fast=True)\n",
    "\n",
    "\n",
    "base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "print(\"Î™®Îç∏ Î°úÎî© ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2. ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Î•º Ï§ÄÎπÑÌï©ÎãàÎã§...\")\n",
    "\n",
    "\n",
    "val_ds = val_ds.shuffle(seed=42) \n",
    "train_ds = train_ds.shuffle()\n",
    "test_ds = test_ds.shuffle()\n",
    "\n",
    "for i in range(10):\n",
    "    ground_truth_answer = test_ds['solutions'][i]\n",
    "    prompt = test_ds['prompt'][i]\n",
    "    image = test_ds['images'][i][0]\n",
    "\n",
    "    inputs = processor(text=prompt, images=[image], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    print(\"\\n4. Î™®Îç∏Ïùò ÎãµÎ≥Ä ÏÉùÏÑ±ÏùÑ ÏãúÏûëÌï©ÎãàÎã§...\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024, \n",
    "            do_sample=False,    \n",
    "        )\n",
    "\n",
    "\n",
    "    output_ids = output_ids[:, inputs['input_ids'].shape[1]:]\n",
    "    generated_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(\"ÏÉùÏÑ± ÏôÑÎ£å!\")\n",
    "    print(f\"‚úÖ Ïã§Ï†ú Ï†ïÎãµ: {ground_truth_answer}\")\n",
    "    print(f\"ü§ñ Î™®Îç∏ ÏÉùÏÑ± ÎãµÎ≥Ä:\\n{generated_text.strip()}\")\n",
    "    print(\"--------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
